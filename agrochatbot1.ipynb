{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6198fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet-pytorch in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: groq in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.37.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: torch in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.17.0+cu118)\n",
      "Requirement already satisfied: pillow in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\koushika\\appdata\\roaming\\python\\python312\\site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\koushika\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchvision) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchvision) (2.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\koushika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet-pytorch groq python-dotenv torch torchvision pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f15092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from typing import Dict, Optional\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb800d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Groq Client Initialized Securely\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "client = Groq()\n",
    "\n",
    "print(\"‚úÖ Groq Client Initialized Securely\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ff6d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASE_KNOWLEDGE = \"\"\"\n",
    "\n",
    "Pepper Bell Bacterial Spot:\n",
    "- Symptoms: Small dark water-soaked spots on leaves and fruits, yellow halos, leaf drop\n",
    "- Cause: Xanthomonas bacteria\n",
    "- Organic Control: Neem oil, copper spray\n",
    "- Chemical Control: Copper fungicide, Streptomycin\n",
    "- Prevention: Disease-free seeds, crop rotation, avoid overhead irrigation\n",
    "\n",
    "Pepper Bell Healthy:\n",
    "- Condition: Plant shows normal green leaves, good growth, no infections\n",
    "- Prevention: Proper irrigation, balanced fertilizer, pest monitoring\n",
    "\n",
    "Potato Early Blight:\n",
    "- Symptoms: Brown concentric rings on lower leaves, yellowing, leaf drop\n",
    "- Cause: Fungus (Alternaria solani)\n",
    "- Organic Control: Neem oil, compost tea\n",
    "- Chemical Control: Mancozeb, Chlorothalonil\n",
    "- Prevention: Crop rotation, remove infected debris\n",
    "\n",
    "Potato Healthy:\n",
    "- Condition: Normal leaf color, strong stems, healthy tuber development\n",
    "- Prevention: Proper drainage, disease-free seed tubers, balanced nutrition\n",
    "\n",
    "‚úÖ Potato Late Blight:\n",
    "- Symptoms: Dark water-soaked lesions on leaves, white mold under leaves, tuber rot\n",
    "- Cause: Phytophthora infestans\n",
    "- Organic Control: Copper oxychloride, neem oil, baking soda spray\n",
    "- Chemical Control: Metalaxyl, Mancozeb, Chlorothalonil\n",
    "- Prevention: Crop rotation, remove infected plants, resistant varieties\n",
    "\n",
    "Tomato Bacterial Spot:\n",
    "- Symptoms: Small dark lesions on leaves and fruit, yellow margins\n",
    "- Cause: Xanthomonas bacteria\n",
    "- Organic Control: Neem oil, copper soap\n",
    "- Chemical Control: Copper fungicides\n",
    "- Prevention: Clean seeds, avoid wet foliage\n",
    "\n",
    "Tomato Early Blight:\n",
    "- Symptoms: Brown rings on leaves, leaf yellowing\n",
    "- Cause: Alternaria solani\n",
    "- Organic Control: Neem oil, compost spray\n",
    "- Chemical Control: Mancozeb, Chlorothalonil\n",
    "- Prevention: Crop rotation, mulch soil\n",
    "\n",
    "Tomato Healthy:\n",
    "- Condition: Bright green leaves, healthy fruit, no lesions\n",
    "- Prevention: Balanced nutrients, regular pruning, pest control\n",
    "\n",
    "Tomato Late Blight:\n",
    "- Symptoms: Black greasy lesions, rapid plant collapse\n",
    "- Cause: Phytophthora infestans\n",
    "- Organic Control: Copper fungicide\n",
    "- Chemical Control: Metalaxyl, Mancozeb\n",
    "- Prevention: Avoid excess moisture, resistant hybrids\n",
    "\n",
    "Tomato Leaf Mold:\n",
    "- Symptoms: Yellow patches on upper leaf, olive-green mold under leaf\n",
    "- Cause: Passalora fulva fungus\n",
    "- Organic Control: Baking soda spray, neem oil\n",
    "- Chemical Control: Chlorothalonil\n",
    "- Prevention: Good air circulation, reduce humidity\n",
    "\n",
    "Tomato Septoria Leaf Spot:\n",
    "- Symptoms: Small brown spots with gray centers, leaf drop\n",
    "- Cause: Septoria lycopersici fungus\n",
    "- Organic Control: Neem oil, compost tea\n",
    "- Chemical Control: Fungicides (Captan, Mancozeb)\n",
    "- Prevention: Remove infected leaves, rotate crops\n",
    "\n",
    "Tomato Spider Mites:\n",
    "- Symptoms: Yellow stippling on leaves, webbing, weak plants\n",
    "- Cause: Two-spotted spider mite pest\n",
    "- Organic Control: Neem oil, water spray, insecticidal soap\n",
    "- Chemical Control: Abamectin\n",
    "- Prevention: Maintain humidity, remove weeds\n",
    "\n",
    "Tomato Target Spot:\n",
    "- Symptoms: Dark circular lesions with yellow borders\n",
    "- Cause: Corynespora cassiicola fungus\n",
    "- Organic Control: Neem oil, bio-fungicide\n",
    "- Chemical Control: Chlorothalonil\n",
    "- Prevention: Spacing plants, avoid leaf wetness\n",
    "\n",
    "Tomato Mosaic Virus:\n",
    "- Symptoms: Mottled leaves, distorted growth\n",
    "- Cause: Viral infection\n",
    "- Control: No cure ‚Äî remove infected plants\n",
    "- Prevention: Disease-free seeds, disinfect tools\n",
    "\n",
    "Tomato Yellow Leaf Curl Virus:\n",
    "- Symptoms: Yellow curled leaves, stunted growth\n",
    "- Cause: Virus transmitted by whiteflies\n",
    "- Organic Control: Neem oil for whitefly control\n",
    "- Chemical Control: Imidacloprid\n",
    "- Prevention: Insect netting, remove infected plants\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2cc7e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSECT_KNOWLEDGE = \"\"\"\n",
    "Aphids:\n",
    "- Damage: Suck sap from leaves\n",
    "- Organic Control: Neem oil, soap spray\n",
    "\n",
    "Whiteflies:\n",
    "- Damage: Transmit plant viruses\n",
    "- Organic Control: Neem oil\n",
    "- Control: Yellow sticky traps, Imidacloprid\n",
    "\n",
    "Spider Mites:\n",
    "- Damage: Yellow speckles, webbing on leaves\n",
    "- Organic Control: Neem oil, water spray\n",
    "- Chemical Control: Abamectin\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f336e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgriculturalRAG:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.knowledge_base = \"\"\n",
    "\n",
    "        # ‚úÖ Safe fallback model list (auto-switch if one fails)\n",
    "        self.models = [\n",
    "            \"llama-3.1-8b-instant\",   # ‚úÖ Fast & currently stable\n",
    "            \"gemma2-9b-it\",           # ‚úÖ Stable instruction model\n",
    "        ]\n",
    "\n",
    "    def load_knowledge_base(self, disease_knowledge, insect_knowledge):\n",
    "        self.knowledge_base = f\"\"\"\n",
    "        DISEASE KNOWLEDGE:\n",
    "        {disease_knowledge}\n",
    "\n",
    "        INSECT KNOWLEDGE:\n",
    "        {insect_knowledge}\n",
    "        \"\"\"\n",
    "        print(\"‚úÖ Knowledge Base Loaded into RAG\")\n",
    "\n",
    "    def query(self, question, verbose=False):\n",
    "        prompt = f\"\"\"\n",
    "You are an expert agricultural plant pathologist.\n",
    "\n",
    "Use the following knowledge to answer carefully and accurately.\n",
    "\n",
    "KNOWLEDGE BASE:\n",
    "{self.knowledge_base}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Give:\n",
    "- Symptoms\n",
    "- Cause\n",
    "- Organic Treatment\n",
    "- Chemical Treatment\n",
    "- Prevention\n",
    "\"\"\"\n",
    "\n",
    "        last_error = None\n",
    "\n",
    "        # ‚úÖ Try models one by one until one works\n",
    "        for model_name in self.models:\n",
    "            try:\n",
    "                if verbose:\n",
    "                    print(f\"üîÑ Trying Groq model: {model_name}\")\n",
    "\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a professional agricultural disease expert.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.2\n",
    "                )\n",
    "\n",
    "                answer = response.choices[0].message.content\n",
    "\n",
    "                return {\n",
    "                    \"answer\": answer,\n",
    "                    \"debug\": {\n",
    "                        \"model_used\": model_name,\n",
    "                        \"sources\": [\"Groq LLM + Custom Knowledge Base\"]\n",
    "                    }\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                last_error = str(e)\n",
    "                if verbose:\n",
    "                    print(f\"‚ùå Model failed: {model_name}\")\n",
    "\n",
    "        # ‚úÖ If all models fail\n",
    "        return {\n",
    "            \"answer\": f\"‚ö†Ô∏è All Groq models failed. Last error:\\n{last_error}\",\n",
    "            \"debug\": {\"model_used\": None, \"sources\": []}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c1c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Knowledge Base Loaded into RAG\n",
      "‚úÖ RAG System Ready\n"
     ]
    }
   ],
   "source": [
    "rag = AgriculturalRAG(client=client)\n",
    "rag.load_knowledge_base(DISEASE_KNOWLEDGE, INSECT_KNOWLEDGE)\n",
    "\n",
    "print(\"‚úÖ RAG System Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a1f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedPlantDiseaseSystem:\n",
    "    \n",
    "    def __init__(self, cnn_model_path: str, rag_system, confidence_threshold: float = 0.75):\n",
    "        self.rag = rag_system\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        checkpoint = torch.load(cnn_model_path, map_location=self.device)\n",
    "        \n",
    "        self.class_names = checkpoint[\"classes\"]\n",
    "        self.cnn_model = EfficientNet.from_pretrained(\n",
    "            'efficientnet-b0', \n",
    "            num_classes=len(self.class_names)\n",
    "        )\n",
    "        self.cnn_model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.cnn_model.to(self.device)\n",
    "        self.cnn_model.eval()\n",
    "        \n",
    "        from torchvision import transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                [0.485, 0.456, 0.406], \n",
    "                [0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        print(\"‚úÖ Integrated system initialized\")\n",
    "        print(\"CNN Classes:\", len(self.class_names))\n",
    "        print(\"Confidence Threshold:\", confidence_threshold)\n",
    "        print(\"Device:\", self.device)\n",
    "    \n",
    "\n",
    "    def predict_disease(self, image_path: str, top_k: int = 3) -> Dict:\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.cnn_model(img_tensor)\n",
    "            probabilities = F.softmax(logits, dim=1)[0]\n",
    "        \n",
    "        top_probs, top_indices = torch.topk(\n",
    "            probabilities, k=min(top_k, len(self.class_names))\n",
    "        )\n",
    "        \n",
    "        top_predictions = [\n",
    "            (self.class_names[idx.item()], prob.item()) \n",
    "            for idx, prob in zip(top_indices, top_probs)\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'top_prediction': top_predictions[0][0],\n",
    "            'confidence': top_predictions[0][1],\n",
    "            'top_k_predictions': top_predictions,\n",
    "            'is_confident': top_predictions[0][1] >= self.confidence_threshold\n",
    "        }\n",
    "    \n",
    "\n",
    "    def diagnose_and_advise(self, image_path: str, user_question: Optional[str] = None, include_alternatives: bool = True) -> Dict:\n",
    "        \n",
    "        prediction = self.predict_disease(image_path)\n",
    "        primary_disease = prediction['top_prediction']\n",
    "        \n",
    "        if user_question:\n",
    "            query = f\"{user_question} (Detected disease: {primary_disease})\"\n",
    "        else:\n",
    "            query = f\"What are the symptoms, causes, and complete treatment plan for {primary_disease}?\"\n",
    "        \n",
    "        advice = self.rag.query(query)\n",
    "        \n",
    "        result = {\n",
    "            'detection': prediction,\n",
    "            'primary_advice': advice,\n",
    "            'alternative_advice': []\n",
    "        }\n",
    "        \n",
    "        if include_alternatives and not prediction['is_confident']:\n",
    "            for disease, conf in prediction['top_k_predictions'][1:]:\n",
    "                alt_advice = self.rag.query(f\"Brief treatment for {disease}\")\n",
    "                result['alternative_advice'].append({\n",
    "                    'disease': disease,\n",
    "                    'confidence': conf,\n",
    "                    'advice': alt_advice\n",
    "                })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "    def print_diagnosis_report(self, result: Dict):\n",
    "        detection = result['detection']\n",
    "        advice = result['primary_advice']\n",
    "        \n",
    "        print(\"\\nü¶† DETECTED DISEASE:\", detection['top_prediction'])\n",
    "        print(\"Confidence:\", f\"{detection['confidence']:.2%}\")\n",
    "        \n",
    "        if not detection['is_confident']:\n",
    "            print(\"\\n‚ö† LOW CONFIDENCE WARNING\")\n",
    "            for disease, conf in detection['top_k_predictions'][1:3]:\n",
    "                print(f\"Alternative: {disease} ({conf:.2%})\")\n",
    "        \n",
    "        print(\"\\nüíä TREATMENT:\")\n",
    "        print(advice['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a7511e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "‚úÖ Integrated system initialized\n",
      "CNN Classes: 15\n",
      "Confidence Threshold: 0.75\n",
      "Device: cuda\n",
      "\n",
      "ü¶† DETECTED DISEASE: Potato___Late_blight\n",
      "Confidence: 91.20%\n",
      "\n",
      "üíä TREATMENT:\n",
      "Based on the provided knowledge base, the disease you are dealing with is Potato Late Blight.\n",
      "\n",
      "Here's the information you requested:\n",
      "\n",
      "**Potato Late Blight:**\n",
      "\n",
      "- **Symptoms:** Dark water-soaked lesions on leaves, white mold under leaves, tuber rot\n",
      "- **Cause:** Phytophthora infestans\n",
      "- **Organic Treatment:** Copper oxychloride, neem oil, baking soda spray\n",
      "- **Chemical Treatment:** Metalaxyl, Mancozeb, Chlorothalonil\n",
      "- **Prevention:** Crop rotation, remove infected plants, resistant varieties\n"
     ]
    }
   ],
   "source": [
    "system = IntegratedPlantDiseaseSystem(\n",
    "    cnn_model_path=\"checkpoints/efficientnet_b0_best.pth\",\n",
    "    rag_system=rag,\n",
    "    confidence_threshold=0.75\n",
    ")\n",
    "\n",
    "result = system.diagnose_and_advise(\n",
    "    image_path=r\"K:\\cnndisease\\00b1f292-23dd-44d4-aad3-c1ffb6a6ad5a___RS_LB 4479.JPG\",\n",
    "    user_question=\"How can I treat this organically?\",\n",
    "    include_alternatives=True\n",
    ")\n",
    "\n",
    "system.print_diagnosis_report(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "440f03c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pepper__bell___Bacterial_spot', 'Pepper__bell___healthy', 'Potato___Early_blight', 'Potato___healthy', 'Potato___Late_blight', 'Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_healthy', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Target_Spot', 'Tomato__Tomato_mosaic_virus', 'Tomato__Tomato_YellowLeaf__Curl_Virus']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(r\"K:\\plant villagee\\PlantVillage\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c904bf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total CNN Classes: 15\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "‚úÖ CNN model rebuilt and loaded\n",
      "‚úÖ Dataset Loaded: K:\\plant villagee\\PlantVillage\n",
      "‚úÖ Total Images Found: 20638\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m y_pred = []\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    674\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    677\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     49\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\datasets\\folder.py:231\u001b[39m, in \u001b[36mDatasetFolder.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    229\u001b[39m sample = \u001b[38;5;28mself\u001b[39m.loader(path)\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:467\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    465\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    466\u001b[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) == \u001b[32m2\u001b[39m):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\koushika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:2321\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2309\u001b[39m         \u001b[38;5;28mself\u001b[39m = (\n\u001b[32m   2310\u001b[39m             \u001b[38;5;28mself\u001b[39m.reduce(factor, box=reduce_box)\n\u001b[32m   2311\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.reduce)\n\u001b[32m   2312\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Image.reduce(\u001b[38;5;28mself\u001b[39m, factor, box=reduce_box)\n\u001b[32m   2313\u001b[39m         )\n\u001b[32m   2314\u001b[39m         box = (\n\u001b[32m   2315\u001b[39m             (box[\u001b[32m0\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2316\u001b[39m             (box[\u001b[32m1\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2317\u001b[39m             (box[\u001b[32m2\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2318\u001b[39m             (box[\u001b[32m3\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2319\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# ‚úÖ FINAL CNN EVALUATION + READINESS CHECK (USING FULL DATASET)\n",
    "# ============================\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ‚úÖ STEP 0: SET YOUR REAL DATASET PATH ‚úÖ (CONFIRMED)\n",
    "# -------------------------------------------------\n",
    "\n",
    "TEST_DIR = r\"K:\\plant villagee\\PlantVillage\"   # ‚úÖ YOUR CONFIRMED REAL PATH\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ‚úÖ STEP 1: LOAD CLASS NAMES + MODEL\n",
    "# -------------------------------------------------\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints/efficientnet_b0_best.pth\", map_location=\"cuda\")\n",
    "\n",
    "class_names = checkpoint[\"classes\"]\n",
    "num_total_classes = len(class_names)\n",
    "\n",
    "print(\"‚úÖ Total CNN Classes:\", num_total_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EfficientNet.from_pretrained(\n",
    "    \"efficientnet-b0\",\n",
    "    num_classes=num_total_classes\n",
    ")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ CNN model rebuilt and loaded\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ‚úÖ STEP 2: BUILD DATA LOADER FROM YOUR DATASET\n",
    "# -------------------------------------------------\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(TEST_DIR, transform=transform)\n",
    "test_loader = DataLoader(full_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"‚úÖ Dataset Loaded:\", TEST_DIR)\n",
    "print(\"‚úÖ Total Images Found:\", len(full_dataset))\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ‚úÖ STEP 3: GENERATE y_true & y_pred\n",
    "# -------------------------------------------------\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(\"‚úÖ Unique Labels Found:\", len(np.unique(y_true)))\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ‚úÖ STEP 4: GENERATE CLASSIFICATION REPORT\n",
    "# -------------------------------------------------\n",
    "\n",
    "report = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    labels=np.unique(y_true),\n",
    "    target_names=[class_names[i] for i in np.unique(y_true)],\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ‚úÖ STEP 5: CNN READINESS FUNCTION\n",
    "# -------------------------------------------------\n",
    "\n",
    "def check_model_readiness(classification_report_dict):\n",
    "    \n",
    "    overall_accuracy = classification_report_dict.get(\"accuracy\", 0)\n",
    "    macro_f1 = classification_report_dict.get(\"macro avg\", {}).get(\"f1-score\", 0)\n",
    "    \n",
    "    poor_classes = []\n",
    "    for class_name, metrics in classification_report_dict.items():\n",
    "        if isinstance(metrics, dict) and \"f1-score\" in metrics:\n",
    "            if metrics[\"f1-score\"] < 0.75:\n",
    "                poor_classes.append((class_name, metrics[\"f1-score\"]))\n",
    "    \n",
    "    if overall_accuracy >= 0.95 and macro_f1 >= 0.93:\n",
    "        status = \"‚úÖ EXCELLENT\"\n",
    "    elif overall_accuracy >= 0.90:\n",
    "        status = \"‚úì GOOD\"\n",
    "    elif overall_accuracy >= 0.85:\n",
    "        status = \"‚ö† ACCEPTABLE\"\n",
    "    else:\n",
    "        status = \"‚ùå NOT READY\"\n",
    "    \n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"overall_accuracy\": overall_accuracy,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"poor_performing_classes\": poor_classes\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ‚úÖ STEP 6: RUN READINESS CHECK\n",
    "# -------------------------------------------------\n",
    "\n",
    "readiness = check_model_readiness(report)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"‚úÖ CNN DEPLOYMENT READINESS\")\n",
    "print(\"==============================\")\n",
    "\n",
    "print(\"MODEL STATUS:\", readiness[\"status\"])\n",
    "print(\"Overall Accuracy:\", round(readiness[\"overall_accuracy\"] * 100, 2), \"%\")\n",
    "print(\"Macro F1 Score:\", round(readiness[\"macro_f1\"], 4))\n",
    "\n",
    "if readiness[\"poor_performing_classes\"]:\n",
    "    print(\"\\n‚ö† WEAK CLASSES (Need Improvement):\")\n",
    "    for cls, f1 in readiness[\"poor_performing_classes\"]:\n",
    "        print(f\"  {cls} ‚Üí F1 = {f1:.3f}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ ALL CLASSES ARE ABOVE MINIMUM F1 THRESHOLD\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# ‚úÖ STEP 7: SAVE REPORT\n",
    "# -------------------------------------------------\n",
    "\n",
    "with open(\"cnn_readiness_report.json\", \"w\") as f:\n",
    "    json.dump(readiness, f, indent=4)\n",
    "\n",
    "print(\"\\n‚úÖ Readiness report saved as: cnn_readiness_report.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
